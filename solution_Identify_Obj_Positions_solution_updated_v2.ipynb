{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Object Positions in Images - YOLO vs VLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepinfra_api_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "google_api_key_g = os.getenv(\"GOOGLE_API_KEY_G\")\n",
    "openai_api_key_g = os.getenv(\"OPENAI_API_KEY_G\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\Gian\\Documents\\Studium\\WirtschaftsInformatik\\DS\\MultimodalInteraction_ObjDet\\images\\table_scene.jpeg: 640x640 1 cup, 2 potted plants, 2 dining tables, 4 books, 3 vases, 117.6ms\n",
      "Speed: 6.3ms preprocess, 117.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\Gian\\Documents\\Studium\\WirtschaftsInformatik\\DS\\MultimodalInteraction_ObjDet\\runs\\detect\\predict13\u001b[0m\n",
      "Bounding boxes for 'cup': [[615.28662109375, 553.3635864257812, 845.9586181640625, 780.4920654296875]]\n"
     ]
    }
   ],
   "source": [
    "# Import YOLO and load a pre-trained model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
    "\n",
    "# Get the class ID for the target object\n",
    "target_object_name = \"cup\"  # Replace with your target object name\n",
    "class_names = model.names\n",
    "target_class_id = next((class_id for class_id, name in class_names.items() if name == target_object_name), None)\n",
    "\n",
    "\n",
    "if target_class_id is not None:\n",
    "    # Perform inference\n",
    "    results = model('images/table_scene.jpeg', save = True)  # Replace with your image path\n",
    "\n",
    "    # Filter bounding boxes for the target object\n",
    "    detections = results[0].boxes\n",
    "    specific_boxes = [\n",
    "        box.xyxy[0].tolist()\n",
    "        for box in detections\n",
    "        if int(box.cls[0]) == target_class_id\n",
    "    ]\n",
    "\n",
    "    print(f\"Bounding boxes for '{target_object_name}': {specific_boxes}\")\n",
    "else:\n",
    "    print(f\"Object name '{target_object_name}' not found in the model's class names.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from PIL import ImageColor\n",
    "\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "#this function is needed to plot bounding boxes on images \n",
    "def plot_bounding_boxes(im, positions, noun_phrase=None):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image with markers for each noun phrase, using PIL, normalized coordinates, and different colors.\n",
    "\n",
    "    Args:\n",
    "        img_path: The path to the image file.\n",
    "        noun_phrases_and_positions: A list of tuples containing the noun phrases\n",
    "         and their positions in normalized [y1 x1 y2 x2] format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = im\n",
    "    width, height = img.size\n",
    "    print(img.size)\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Try to load a TrueType font for larger labels, fallback to default\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=24)\n",
    "    except Exception:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Define a list of colors\n",
    "    colors = [\n",
    "        'red',\n",
    "        'green',\n",
    "        'blue',\n",
    "        'yellow',\n",
    "        'orange',\n",
    "    ] + additional_colors\n",
    "\n",
    "    for i, item in enumerate(positions):\n",
    "        # Support entries of the form: [y1,x1,y2,x2] or (label, [y1,x1,y2,x2])\n",
    "        if isinstance(item, (list, tuple)) and len(item) == 2 and isinstance(item[0], str):\n",
    "            label = item[0]\n",
    "            try:\n",
    "                y1, x1, y2, x2 = item[1]\n",
    "            except Exception:\n",
    "                continue\n",
    "        else:\n",
    "            label = noun_phrase if noun_phrase is not None else \"\"\n",
    "            try:\n",
    "                y1, x1, y2, x2 = item\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        abs_x1 = int(x1/1000 * width)\n",
    "        abs_y1 = int(y1/1000 * height)\n",
    "        abs_x2 = int(x2/1000 * width)\n",
    "        abs_y2 = int(y2/1000 * height)\n",
    "\n",
    "        draw.rectangle(((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4)\n",
    "        if label:\n",
    "            draw.text((abs_x1 + 8, abs_y1 + 6), str(label), fill=color, font=font)\n",
    "\n",
    "    # Display the image\n",
    "    img.show()\n",
    "\n",
    "# if the boxes coordinates are output not as json but as text, should be parsed first\n",
    "def parse_list_boxes(text):\n",
    "  result = []\n",
    "  for line in text.strip().splitlines():\n",
    "    # Extract the numbers from the line, remove brackets and split by comma\n",
    "    try:\n",
    "      numbers = line.split('[')[1].split(']')[0].split(',')\n",
    "    except:\n",
    "      numbers =  line.split('- ')[1].split(',')\n",
    "\n",
    "    # Convert the numbers to integers and append to the result\n",
    "    result.append([int(num.strip()) for num in numbers])\n",
    "\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM (1): OPEN-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import base64\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "load_dotenv()\n",
    "openAIclient = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "img = \"images/table_scene.jpeg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "import json\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Detect if there is a cup in the image and reutrn its coordinates as a list in the format [ymin,xmin, ymax, xmax], normalize the coordinate to 0-1000. Just output the list in json.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "response = str(completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load answer in a python dict and/or parse the json before if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coordinates': [555, 700, 780, 825]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now parse and plot bounding boxes. Consider the format the utils function want the bounding boxes to be plotted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[555, 700, 780, 825]]\n",
      "(1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "#boxes= parse_list_boxes(str(detection['coordinates'])) #depending whether you managed to output a json or not. If your output is still in \"str\" form you might need to parse it with this util function\n",
    "boxes= [detection['coordinates']]\n",
    "print(boxes)\n",
    "plot_bounding_boxes(Image.open(img), positions=list(boxes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM (2): GEMINI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dotenv import load_dotenv  \n",
    "from google import genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client(api_key=google_api_key_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\"box_2d\": [538, 597, 757, 824], \"label\": \"cup\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[\n",
    "        im,\n",
    "        (\n",
    "            \"Detect if there is a cup in the image and reutrn its coordinates as a list in the format [ymin,xmin, ymax, xmax], normalize the coordinate to 0-1000. answer should be json format only\"\n",
    "        ),\n",
    "        \n",
    "    ],\n",
    "    config={\"response_mime_type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! Output is a list! because I asked for it in the prompt - either you leave it and take account in parsing, or you remove the word \"list\" in prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'box_2d': [538, 597, 757, 824], 'label': 'cup'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "#boxes= parse_list_boxes(response.text) # same comment as above\n",
    "box_coord = [json.loads(response.text)[0]['box_2d']] #due to the fact it is a list, otherwise this line might change\n",
    "plot_bounding_boxes(im,positions=list(box_coord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "data = json.loads(response.text)\n",
    "entry = data[0]\n",
    "box_coord = [entry['box_2d']]\n",
    "label = entry.get('label')   # 'cup'\n",
    "plot_bounding_boxes(im, positions=box_coord, noun_phrase=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple objects at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"label\": \"cup\",\n",
      "    \"box_2d\": [541, 601, 764, 825]\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"plant\",\n",
      "    \"box_2d\": [20, 600, 617, 989]\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"book\",\n",
      "    \"box_2d\": [574, 0, 878, 590]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Prompt Gemini to detect multiple objects (cup, plant, book)\n",
    "im = Image.open(img)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[\n",
    "        im,\n",
    "        (\n",
    "            \"Detect whether there is a cup, plant, and book in the image and return a JSON array of detections only one detection per object. \"\n",
    "            \"Each detection must be an object with keys 'label' and 'box_2d' where 'box_2d' is [ymin,xmin,ymax,xmax] \"\n",
    "            \"normalized to 0-1000. ONLY return valid JSON (no explanatory text).\"\n",
    "        ),\n",
    "    ],\n",
    "    config={\"response_mime_type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'cup', 'box_2d': [541, 601, 764, 825]},\n",
       " {'label': 'plant', 'box_2d': [20, 600, 617, 989]},\n",
       " {'label': 'book', 'box_2d': [574, 0, 878, 590]}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected: [('cup', [541, 601, 764, 825]), ('plant', [20, 600, 617, 989]), ('book', [574, 0, 878, 590])]\n",
      "(1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# Parse the model response (must run the prompt cell first)\n",
    "try:\n",
    "    data = json.loads(response.text)\n",
    "except Exception as e:\n",
    "    print(\"Failed to parse response.text:\", e)\n",
    "    data = []\n",
    "\n",
    "# Build labeled positions expected by the util: (label, box)\n",
    "positions = [(entry.get('label', ''), entry.get('box_2d')) for entry in data if entry.get('box_2d')]\n",
    "\n",
    "if positions:\n",
    "    print('Detected:', [(p[0], p[1]) for p in positions])\n",
    "    plot_bounding_boxes(Image.open(img), positions=positions)\n",
    "else:\n",
    "    print('No detections parsed from response.text')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
