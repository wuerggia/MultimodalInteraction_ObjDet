{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Object Positions in Images - YOLO vs VLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepinfra_api_key = os.getenv(\"DEEPINFRA_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "google_api_key_g = os.getenv(\"GOOGLE_API_KEY_G\")\n",
    "openai_api_key_g = os.getenv(\"OPENAI_API_KEY_G\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\Gian\\Documents\\Studium\\WirtschaftsInformatik\\DS\\MultimodalInteraction_ObjDet\\images\\table_scene.jpeg: 640x640 1 cup, 2 potted plants, 2 dining tables, 4 books, 3 vases, 245.8ms\n",
      "Speed: 12.8ms preprocess, 245.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mC:\\Users\\Gian\\Documents\\Studium\\WirtschaftsInformatik\\DS\\MultimodalInteraction_ObjDet\\runs\\detect\\predict4\u001b[0m\n",
      "Bounding boxes for 'cup': [[615.28662109375, 553.3635864257812, 845.9586181640625, 780.4920654296875]]\n"
     ]
    }
   ],
   "source": [
    "# Import YOLO and load a pre-trained model\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
    "\n",
    "# Get the class ID for the target object\n",
    "target_object_name = \"cup\"  # Replace with your target object name\n",
    "class_names = model.names\n",
    "target_class_id = next((class_id for class_id, name in class_names.items() if name == target_object_name), None)\n",
    "\n",
    "\n",
    "if target_class_id is not None:\n",
    "    # Perform inference\n",
    "    results = model('images/table_scene.jpeg', save = True)  # Replace with your image path\n",
    "\n",
    "    # Filter bounding boxes for the target object\n",
    "    detections = results[0].boxes\n",
    "    specific_boxes = [\n",
    "        box.xyxy[0].tolist()\n",
    "        for box in detections\n",
    "        if int(box.cls[0]) == target_class_id\n",
    "    ]\n",
    "\n",
    "    print(f\"Bounding boxes for '{target_object_name}': {specific_boxes}\")\n",
    "else:\n",
    "    print(f\"Object name '{target_object_name}' not found in the model's class names.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import re\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL import ImageColor\n",
    "\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "#this function is needed to plot bounding boxes on images \n",
    "def plot_bounding_boxes(im, positions):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image with markers for each noun phrase, using PIL, normalized coordinates, and different colors.\n",
    "\n",
    "    Args:\n",
    "        img_path: The path to the image file.\n",
    "        noun_phrases_and_positions: A list of tuples containing the noun phrases\n",
    "         and their positions in normalized [y1 x1 y2 x2] format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = im\n",
    "    width, height = img.size\n",
    "    print(img.size)\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Define a list of colors\n",
    "    colors = [\n",
    "    'red',\n",
    "    'green',\n",
    "    'blue',\n",
    "    'yellow',\n",
    "    'orange',\n",
    "    ] + additional_colors\n",
    "\n",
    "    # Iterate over the noun phrases and their positions\n",
    "    for i, ((y1, x1, y2, x2)) in enumerate(positions):\n",
    "        # Select a color from the list\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        # Convert normalized coordinates to absolute coordinates\n",
    "        abs_x1 = int(x1/1000 * width)\n",
    "        abs_y1 = int(y1/1000 * height)\n",
    "        abs_x2 = int(x2/1000 * width)\n",
    "        abs_y2 = int(y2/1000 * height)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        draw.rectangle(\n",
    "            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n",
    "        )\n",
    "\n",
    "        # Draw the text\n",
    "        #draw.text((abs_x1 + 8, abs_y1 + 6), noun_phrase, fill=color)\n",
    "\n",
    "    # Display the image\n",
    "    img.show()\n",
    "\n",
    "# if the boxes coordinates are output not as json but as text, should be parsed first\n",
    "def parse_list_boxes(text):\n",
    "  result = []\n",
    "  for line in text.strip().splitlines():\n",
    "    # Extract the numbers from the line, remove brackets and split by comma\n",
    "    try:\n",
    "      numbers = line.split('[')[1].split(']')[0].split(',')\n",
    "    except:\n",
    "      numbers =  line.split('- ')[1].split(',')\n",
    "\n",
    "    # Convert the numbers to integers and append to the result\n",
    "    result.append([int(num.strip()) for num in numbers])\n",
    "\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM (1): OPEN-AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import base64\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "load_dotenv()\n",
    "openAIclient = openai.OpenAI(api_key=openai_api_key)\n",
    "\n",
    "img = \"images/table_scene.jpeg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic call to gpt4 with prompt and image\n",
    "\n",
    "import json\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Detect if there is a cup in the image and reutrn its coordinates as a list in the format [ymin,xmin, ymax, xmax], normalize the coordinate to 0-1000. Just output the list in json.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    ")\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "response = str(completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load answer in a python dict and/or parse the json before if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = json.loads(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coordinates': [560, 710, 860, 890]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now parse and plot bounding boxes. Consider the format the utils function want the bounding boxes to be plotted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[560, 710, 860, 890]]\n",
      "(1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "#boxes= parse_list_boxes(str(detection['coordinates'])) #depending whether you managed to output a json or not. If your output is still in \"str\" form you might need to parse it with this util function\n",
    "boxes= [detection['coordinates']]\n",
    "print(boxes)\n",
    "plot_bounding_boxes(Image.open(img), positions=list(boxes))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM (2): GEMINI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dotenv import load_dotenv  \n",
    "from google import genai\n",
    "from PIL import Image\n",
    "\n",
    "load_dotenv()\n",
    "client = genai.Client(api_key=google_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\"box_2d\": [538, 590, 750, 810], \"label\": \"cup\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[\n",
    "        im,\n",
    "        (\n",
    "            \"Detect if there is a cup in the image and reutrn its coordinates as a list in the format [ymin,xmin, ymax, xmax], normalize the coordinate to 0-1000. answer should be json format only\"\n",
    "        ),\n",
    "        \n",
    "    ],\n",
    "    config={\"response_mime_type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah! Output is a list! because I asked for it in the prompt - either you leave it and take account in parsing, or you remove the word \"list\" in prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'box_2d': [538, 590, 750, 810], 'label': 'cup'}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "#boxes= parse_list_boxes(response.text) # same comment as above\n",
    "box_coord = [json.loads(response.text)[0]['box_2d']] #due to the fact it is a list, otherwise this line might change\n",
    "plot_bounding_boxes(im,positions=list(box_coord))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
