{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection  - YOLO & OWL-ViT\n",
    "This tutorial demonstrates how to use YOLO (You Only Look Once) from the [Ultralytics](https://github.com/ultralytics/yolov5) library for object detection. It includes steps for:\n",
    "\n",
    "- Running object detection inference on images/videos\n",
    "- Fine-tuning YOLO for custom datasets\n",
    "- Comparing YOLO with OWl-VIT for zero-shot learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perform Object Detection Inference\n",
    "First thing We'll use YOLOv8 from Ultralyics for object detection on a sample image.\n",
    "We aim to utilize the pre-trained YOLOv8 model to detect objects in a sample image. This involves loading the model, providing an image for input, and interpreting the model's predictions.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Inference**: The process of using a trained model to make predictions on new data.\n",
    "- **YOLOv8**: A state-of-the-art version of the YOLO (You Only Look Once) architecture, known for its speed and accuracy in object detection tasks.\n",
    "\n",
    "**Steps:**\n",
    "1. Load the YOLOv8 model using the Ultralytics library.\n",
    "2. Perform inference on a sample image to detect objects.\n",
    "3. Visualize the results, including bounding boxes and class labels.\n",
    "\n",
    "**Support Material:**\n",
    "- https://docs.ultralytics.com/models/yolov8/\n",
    "- https://docs.ultralytics.com/tasks/detect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspaces/MultimodalInteraction_ObjDet/images/street_scene.jpg: 384x640 13 persons, 1 bicycle, 9 cars, 2 motorcycles, 1 traffic light, 1 bench, 4 birds, 1 handbag, 1 potted plant, 129.3ms\n",
      "Speed: 3.8ms preprocess, 129.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 2.,  0.,  0.,  0., 58.,  0.,  2.,  9.,  0., 14.,  0.,  3.,  0.,  1.,  2., 14., 14.,  2.,  0.,  2.,  2.,  0.,  0., 26.,  0.,  3.,  2.,  0.,  0.,  2., 14., 13.,  2.])\n",
      "conf: tensor([0.9098, 0.9041, 0.9005, 0.8934, 0.8477, 0.8331, 0.8173, 0.7737, 0.7585, 0.7313, 0.6779, 0.6606, 0.6198, 0.5686, 0.5105, 0.5057, 0.5043, 0.4675, 0.4564, 0.4517, 0.4201, 0.4165, 0.4037, 0.4015, 0.3767, 0.3745, 0.3659, 0.3221, 0.3095, 0.3049, 0.2999, 0.2989, 0.2811])\n",
      "data: tensor([[9.5592e-01, 3.6429e+02, 6.0592e+02, 6.1893e+02, 9.0984e-01, 2.0000e+00],\n",
      "        [1.1789e+03, 4.2397e+02, 1.4806e+03, 8.6406e+02, 9.0414e-01, 0.0000e+00],\n",
      "        [1.5657e+03, 2.9039e+02, 1.7113e+03, 6.7585e+02, 9.0047e-01, 0.0000e+00],\n",
      "        [1.0586e+03, 4.0629e+02, 1.3231e+03, 7.3748e+02, 8.9344e-01, 0.0000e+00],\n",
      "        [1.9016e+02, 6.9791e+02, 4.1024e+02, 9.5744e+02, 8.4768e-01, 5.8000e+01],\n",
      "        [4.1516e+02, 5.8301e+02, 6.5140e+02, 9.2680e+02, 8.3306e-01, 0.0000e+00],\n",
      "        [3.7321e+02, 3.2695e+02, 6.1556e+02, 4.2069e+02, 8.1727e-01, 2.0000e+00],\n",
      "        [1.1758e+03, 3.8894e+01, 1.2183e+03, 1.5122e+02, 7.7372e-01, 9.0000e+00],\n",
      "        [9.3326e+02, 3.1249e+02, 1.0399e+03, 5.4408e+02, 7.5851e-01, 0.0000e+00],\n",
      "        [1.1763e+03, 9.2910e+02, 1.2933e+03, 1.0222e+03, 7.3131e-01, 1.4000e+01],\n",
      "        [5.9237e+02, 7.1440e+02, 1.1320e+03, 9.9478e+02, 6.7793e-01, 0.0000e+00],\n",
      "        [1.0581e+03, 3.7448e+02, 1.1869e+03, 5.0788e+02, 6.6059e-01, 3.0000e+00],\n",
      "        [7.4075e+02, 3.0132e+02, 8.6129e+02, 5.2405e+02, 6.1981e-01, 0.0000e+00],\n",
      "        [7.0015e+02, 4.1289e+02, 8.8419e+02, 5.7394e+02, 5.6856e-01, 1.0000e+00],\n",
      "        [5.7998e+02, 3.4540e+02, 7.9575e+02, 4.6612e+02, 5.1050e-01, 2.0000e+00],\n",
      "        [1.1354e+03, 8.4833e+02, 1.2484e+03, 9.4193e+02, 5.0570e-01, 1.4000e+01],\n",
      "        [9.5483e+02, 6.2117e+02, 9.9584e+02, 6.7102e+02, 5.0429e-01, 1.4000e+01],\n",
      "        [1.6384e+03, 3.6660e+02, 1.7917e+03, 4.9285e+02, 4.6751e-01, 2.0000e+00],\n",
      "        [1.3502e+03, 3.4881e+02, 1.3859e+03, 4.3120e+02, 4.5638e-01, 0.0000e+00],\n",
      "        [5.7960e+02, 3.4165e+02, 8.8617e+02, 4.6188e+02, 4.5173e-01, 2.0000e+00],\n",
      "        [1.6945e+03, 3.6793e+02, 1.7915e+03, 4.9295e+02, 4.2015e-01, 2.0000e+00],\n",
      "        [1.4419e+03, 3.4569e+02, 1.4719e+03, 4.4234e+02, 4.1645e-01, 0.0000e+00],\n",
      "        [1.0824e+03, 3.2515e+02, 1.1799e+03, 4.8252e+02, 4.0370e-01, 0.0000e+00],\n",
      "        [9.3231e+02, 3.7889e+02, 9.9831e+02, 4.4572e+02, 4.0154e-01, 2.6000e+01],\n",
      "        [1.1151e+03, 3.2569e+02, 1.1802e+03, 4.7567e+02, 3.7672e-01, 0.0000e+00],\n",
      "        [7.0181e+02, 4.0279e+02, 8.8495e+02, 5.7466e+02, 3.7447e-01, 3.0000e+00],\n",
      "        [1.5457e+03, 3.5707e+02, 1.7918e+03, 4.9037e+02, 3.6586e-01, 2.0000e+00],\n",
      "        [1.3430e+03, 3.4528e+02, 1.3785e+03, 4.3064e+02, 3.2208e-01, 0.0000e+00],\n",
      "        [1.4327e+03, 3.5082e+02, 1.4673e+03, 4.4290e+02, 3.0946e-01, 0.0000e+00],\n",
      "        [1.5333e+03, 3.7740e+02, 1.6210e+03, 4.8554e+02, 3.0489e-01, 2.0000e+00],\n",
      "        [1.0274e+03, 6.0857e+02, 1.0570e+03, 6.5136e+02, 2.9992e-01, 1.4000e+01],\n",
      "        [1.2430e+03, 6.1112e+02, 1.7259e+03, 1.0190e+03, 2.9888e-01, 1.3000e+01],\n",
      "        [8.6543e+02, 3.5008e+02, 9.4009e+02, 3.8912e+02, 2.8114e-01, 2.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (1024, 1792)\n",
      "shape: torch.Size([33, 6])\n",
      "xywh: tensor([[ 303.4360,  491.6105,  604.9601,  254.6311],\n",
      "        [1329.7615,  644.0153,  301.6866,  440.0985],\n",
      "        [1638.5154,  483.1179,  145.6616,  385.4641],\n",
      "        [1190.8245,  571.8881,  264.4564,  331.1922],\n",
      "        [ 300.1982,  827.6760,  220.0836,  259.5260],\n",
      "        [ 533.2831,  754.9047,  236.2435,  343.7872],\n",
      "        [ 494.3851,  373.8213,  242.3596,   93.7392],\n",
      "        [1197.0276,   95.0568,   42.4672,  112.3248],\n",
      "        [ 986.5753,  428.2856,  106.6277,  231.5882],\n",
      "        [1234.8212,  975.6396,  117.0117,   93.0833],\n",
      "        [ 862.1802,  854.5883,  539.6152,  280.3784],\n",
      "        [1122.5028,  441.1806,  128.7488,  133.3917],\n",
      "        [ 801.0178,  412.6852,  120.5370,  222.7268],\n",
      "        [ 792.1693,  493.4146,  184.0450,  161.0409],\n",
      "        [ 687.8665,  405.7602,  215.7759,  120.7179],\n",
      "        [1191.9240,  895.1294,  112.9792,   93.6048],\n",
      "        [ 975.3347,  646.0970,   41.0160,   49.8470],\n",
      "        [1715.0410,  429.7282,  153.3424,  126.2471],\n",
      "        [1368.0439,  390.0021,   35.7831,   82.3865],\n",
      "        [ 732.8855,  401.7635,  306.5644,  120.2286],\n",
      "        [1743.0010,  430.4393,   97.0337,  125.0183],\n",
      "        [1456.9026,  394.0114,   30.0504,   96.6480],\n",
      "        [1131.1941,  403.8324,   97.5084,  157.3738],\n",
      "        [ 965.3070,  412.3027,   65.9979,   66.8322],\n",
      "        [1147.6650,  400.6788,   65.0867,  149.9765],\n",
      "        [ 793.3782,  488.7257,  183.1342,  171.8770],\n",
      "        [1668.7405,  423.7186,  246.1128,  133.3057],\n",
      "        [1360.7778,  387.9605,   35.5201,   85.3629],\n",
      "        [1449.9895,  396.8608,   34.6515,   92.0872],\n",
      "        [1577.1621,  431.4725,   87.7750,  108.1397],\n",
      "        [1042.2415,  629.9612,   29.6053,   42.7921],\n",
      "        [1484.4595,  815.0737,  482.9520,  407.9140],\n",
      "        [ 902.7610,  369.6018,   74.6652,   39.0404]])\n",
      "xywhn: tensor([[0.1693, 0.4801, 0.3376, 0.2487],\n",
      "        [0.7421, 0.6289, 0.1684, 0.4298],\n",
      "        [0.9144, 0.4718, 0.0813, 0.3764],\n",
      "        [0.6645, 0.5585, 0.1476, 0.3234],\n",
      "        [0.1675, 0.8083, 0.1228, 0.2534],\n",
      "        [0.2976, 0.7372, 0.1318, 0.3357],\n",
      "        [0.2759, 0.3651, 0.1352, 0.0915],\n",
      "        [0.6680, 0.0928, 0.0237, 0.1097],\n",
      "        [0.5505, 0.4182, 0.0595, 0.2262],\n",
      "        [0.6891, 0.9528, 0.0653, 0.0909],\n",
      "        [0.4811, 0.8346, 0.3011, 0.2738],\n",
      "        [0.6264, 0.4308, 0.0718, 0.1303],\n",
      "        [0.4470, 0.4030, 0.0673, 0.2175],\n",
      "        [0.4421, 0.4819, 0.1027, 0.1573],\n",
      "        [0.3839, 0.3963, 0.1204, 0.1179],\n",
      "        [0.6651, 0.8741, 0.0630, 0.0914],\n",
      "        [0.5443, 0.6310, 0.0229, 0.0487],\n",
      "        [0.9571, 0.4197, 0.0856, 0.1233],\n",
      "        [0.7634, 0.3809, 0.0200, 0.0805],\n",
      "        [0.4090, 0.3923, 0.1711, 0.1174],\n",
      "        [0.9727, 0.4204, 0.0541, 0.1221],\n",
      "        [0.8130, 0.3848, 0.0168, 0.0944],\n",
      "        [0.6312, 0.3944, 0.0544, 0.1537],\n",
      "        [0.5387, 0.4026, 0.0368, 0.0653],\n",
      "        [0.6404, 0.3913, 0.0363, 0.1465],\n",
      "        [0.4427, 0.4773, 0.1022, 0.1678],\n",
      "        [0.9312, 0.4138, 0.1373, 0.1302],\n",
      "        [0.7594, 0.3789, 0.0198, 0.0834],\n",
      "        [0.8091, 0.3876, 0.0193, 0.0899],\n",
      "        [0.8801, 0.4214, 0.0490, 0.1056],\n",
      "        [0.5816, 0.6152, 0.0165, 0.0418],\n",
      "        [0.8284, 0.7960, 0.2695, 0.3984],\n",
      "        [0.5038, 0.3609, 0.0417, 0.0381]])\n",
      "xyxy: tensor([[9.5592e-01, 3.6429e+02, 6.0592e+02, 6.1893e+02],\n",
      "        [1.1789e+03, 4.2397e+02, 1.4806e+03, 8.6406e+02],\n",
      "        [1.5657e+03, 2.9039e+02, 1.7113e+03, 6.7585e+02],\n",
      "        [1.0586e+03, 4.0629e+02, 1.3231e+03, 7.3748e+02],\n",
      "        [1.9016e+02, 6.9791e+02, 4.1024e+02, 9.5744e+02],\n",
      "        [4.1516e+02, 5.8301e+02, 6.5140e+02, 9.2680e+02],\n",
      "        [3.7321e+02, 3.2695e+02, 6.1556e+02, 4.2069e+02],\n",
      "        [1.1758e+03, 3.8894e+01, 1.2183e+03, 1.5122e+02],\n",
      "        [9.3326e+02, 3.1249e+02, 1.0399e+03, 5.4408e+02],\n",
      "        [1.1763e+03, 9.2910e+02, 1.2933e+03, 1.0222e+03],\n",
      "        [5.9237e+02, 7.1440e+02, 1.1320e+03, 9.9478e+02],\n",
      "        [1.0581e+03, 3.7448e+02, 1.1869e+03, 5.0788e+02],\n",
      "        [7.4075e+02, 3.0132e+02, 8.6129e+02, 5.2405e+02],\n",
      "        [7.0015e+02, 4.1289e+02, 8.8419e+02, 5.7394e+02],\n",
      "        [5.7998e+02, 3.4540e+02, 7.9575e+02, 4.6612e+02],\n",
      "        [1.1354e+03, 8.4833e+02, 1.2484e+03, 9.4193e+02],\n",
      "        [9.5483e+02, 6.2117e+02, 9.9584e+02, 6.7102e+02],\n",
      "        [1.6384e+03, 3.6660e+02, 1.7917e+03, 4.9285e+02],\n",
      "        [1.3502e+03, 3.4881e+02, 1.3859e+03, 4.3120e+02],\n",
      "        [5.7960e+02, 3.4165e+02, 8.8617e+02, 4.6188e+02],\n",
      "        [1.6945e+03, 3.6793e+02, 1.7915e+03, 4.9295e+02],\n",
      "        [1.4419e+03, 3.4569e+02, 1.4719e+03, 4.4234e+02],\n",
      "        [1.0824e+03, 3.2515e+02, 1.1799e+03, 4.8252e+02],\n",
      "        [9.3231e+02, 3.7889e+02, 9.9831e+02, 4.4572e+02],\n",
      "        [1.1151e+03, 3.2569e+02, 1.1802e+03, 4.7567e+02],\n",
      "        [7.0181e+02, 4.0279e+02, 8.8495e+02, 5.7466e+02],\n",
      "        [1.5457e+03, 3.5707e+02, 1.7918e+03, 4.9037e+02],\n",
      "        [1.3430e+03, 3.4528e+02, 1.3785e+03, 4.3064e+02],\n",
      "        [1.4327e+03, 3.5082e+02, 1.4673e+03, 4.4290e+02],\n",
      "        [1.5333e+03, 3.7740e+02, 1.6210e+03, 4.8554e+02],\n",
      "        [1.0274e+03, 6.0857e+02, 1.0570e+03, 6.5136e+02],\n",
      "        [1.2430e+03, 6.1112e+02, 1.7259e+03, 1.0190e+03],\n",
      "        [8.6543e+02, 3.5008e+02, 9.4009e+02, 3.8912e+02]])\n",
      "xyxyn: tensor([[5.3344e-04, 3.5576e-01, 3.3812e-01, 6.0442e-01],\n",
      "        [6.5788e-01, 4.1403e-01, 8.2623e-01, 8.4381e-01],\n",
      "        [8.7371e-01, 2.8358e-01, 9.5499e-01, 6.6001e-01],\n",
      "        [5.9073e-01, 3.9677e-01, 7.3831e-01, 7.2020e-01],\n",
      "        [1.0611e-01, 6.8156e-01, 2.2893e-01, 9.3500e-01],\n",
      "        [2.3167e-01, 5.6935e-01, 3.6351e-01, 9.0508e-01],\n",
      "        [2.0826e-01, 3.1929e-01, 3.4351e-01, 4.1083e-01],\n",
      "        [6.5614e-01, 3.7983e-02, 6.7983e-01, 1.4767e-01],\n",
      "        [5.2079e-01, 3.0517e-01, 5.8030e-01, 5.3133e-01],\n",
      "        [6.5643e-01, 9.0732e-01, 7.2172e-01, 9.9822e-01],\n",
      "        [3.3057e-01, 6.9766e-01, 6.3169e-01, 9.7146e-01],\n",
      "        [5.9047e-01, 3.6571e-01, 6.6232e-01, 4.9597e-01],\n",
      "        [4.1336e-01, 2.9426e-01, 4.8063e-01, 5.1177e-01],\n",
      "        [3.9071e-01, 4.0322e-01, 4.9341e-01, 5.6048e-01],\n",
      "        [3.2365e-01, 3.3731e-01, 4.4406e-01, 4.5519e-01],\n",
      "        [6.3361e-01, 8.2844e-01, 6.9666e-01, 9.1986e-01],\n",
      "        [5.3283e-01, 6.0661e-01, 5.5572e-01, 6.5529e-01],\n",
      "        [9.1427e-01, 3.5801e-01, 9.9984e-01, 4.8130e-01],\n",
      "        [7.5343e-01, 3.4063e-01, 7.7340e-01, 4.2109e-01],\n",
      "        [3.2344e-01, 3.3364e-01, 4.9451e-01, 4.5105e-01],\n",
      "        [9.4558e-01, 3.5931e-01, 9.9973e-01, 4.8139e-01],\n",
      "        [8.0462e-01, 3.3759e-01, 8.2139e-01, 4.3197e-01],\n",
      "        [6.0404e-01, 3.1752e-01, 6.5845e-01, 4.7121e-01],\n",
      "        [5.2026e-01, 3.7001e-01, 5.5709e-01, 4.3527e-01],\n",
      "        [6.2228e-01, 3.1806e-01, 6.5860e-01, 4.6452e-01],\n",
      "        [3.9164e-01, 3.9335e-01, 4.9383e-01, 5.6120e-01],\n",
      "        [8.6255e-01, 3.4870e-01, 9.9989e-01, 4.7888e-01],\n",
      "        [7.4945e-01, 3.3719e-01, 7.6927e-01, 4.2055e-01],\n",
      "        [7.9948e-01, 3.4259e-01, 8.1881e-01, 4.3252e-01],\n",
      "        [8.5562e-01, 3.6856e-01, 9.0460e-01, 4.7416e-01],\n",
      "        [5.7335e-01, 5.9430e-01, 5.8987e-01, 6.3609e-01],\n",
      "        [6.9363e-01, 5.9679e-01, 9.6313e-01, 9.9515e-01],\n",
      "        [4.8294e-01, 3.4188e-01, 5.2461e-01, 3.8000e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Import YOLO and load a pre-trained model\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 pre-trained model\n",
    "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
    "\n",
    "# Run inference on a sample image\n",
    "\n",
    "results = model('images/street_scene.jpg', save = False)  # Displays image with detections\n",
    "\n",
    "for result in results:\n",
    "    print(result.boxes)  # Boxes object for bounding box outputs\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning YOLO on Custom Dataset\n",
    "Fine-tuning YOLO requires a dataset formatted in the YOLO format. We'll use a small public dataset for demonstration.\n",
    "We will adapt the pre-trained YOLO model to a custom dataset. This process, known as fine-tuning, enables YOLO to specialize in detecting specific objects not included in its original training.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Fine-tuning**: Adapting a pre-trained model to new data by continuing the training process.\n",
    "- **Custom Dataset**: A dataset that contains specific objects relevant to a new application, different from those YOLO was trained on (e.g. https://docs.ultralytics.com/datasets/detect/signature/.)\n",
    "\n",
    "**Steps:**\n",
    "1. Prepare the custom dataset by organizing images and labels in the required format.\n",
    "2. Configure the YOLO training pipeline.\n",
    "3. Train the model and evaluate its performance.\n",
    "\n",
    "**Support Material:** \n",
    "- https://docs.ultralytics.com/modes/train/\n",
    "- https://docs.ultralytics.com/modes/val/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample dataset (e.g., Signature)\n",
    "!wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/signature.zip\n",
    "!unzip -q signature.zip -d ./datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.234 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.39 üöÄ Python-3.10.19 torch-2.9.1+cu128 CPU (Intel Xeon Platinum 8370C 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=./datasets/signature.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "\n",
      "Dataset 'datasets/signature.yaml' images not found ‚ö†Ô∏è, missing path '/workspaces/MultimodalInteraction_ObjDet/datasets/signature/images/val'\n",
      "Downloading https://ultralytics.com/assets/signature.zip to '/workspaces/MultimodalInteraction_ObjDet/datasets/signature.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.3M/11.3M [00:00<00:00, 348MB/s]\n",
      "Unzipping /workspaces/MultimodalInteraction_ObjDet/datasets/signature.zip to /workspaces/MultimodalInteraction_ObjDet/datasets/signature...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 364/364 [00:00<00:00, 3151.74file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset download success ‚úÖ (0.4s), saved to \u001b[1m/workspaces/MultimodalInteraction_ObjDet/datasets\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/vscode/.config/Ultralytics/Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 75.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 58/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /workspaces/MultimodalInteraction_ObjDet/datasets/signature/labels/train... 143 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [00:00<00:00, 2006.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /workspaces/MultimodalInteraction_ObjDet/datasets/signature/labels/train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/vscode/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /workspaces/MultimodalInteraction_ObjDet/datasets/signature/labels/val... 35 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:00<00:00, 3352.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /workspaces/MultimodalInteraction_ObjDet/datasets/signature/labels/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "       1/10         0G      2.297      4.778      2.372         15        640: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [01:14<00:00,  8.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         35    0.00114      0.343    0.00281   0.000597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G       1.68      3.793      1.786         16        640:  11%|‚ñà         | 1/9 [00:07<01:01,  7.66s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train YOLO on the dataset\n",
    "results = model.train(data='./datasets/signature.yaml', epochs=10, imgsz=640, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspaces/MultimodalInteraction_ObjDet/images/example_signature.jpg: 640x480 (no detections), 118.5ms\n",
      "Speed: 2.9ms preprocess, 118.5ms inference, 0.4ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"runs/detect/train/weights/best.pt\")  # load a custom model, check the path depending on your output before!!\n",
    "\n",
    "# Predict with the model\n",
    "results = model.predict(\"images/example_signature.jpg\", conf=0.75) #check params if you need to improve detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Learning with OWL-ViT\n",
    "Switch to `OWL-ViT` to see how it performs with zero-shot learning capabilities. Zero-shot means detecting objects without prior specific training.\n",
    "\n",
    "OWL-ViT (Open Vocabulary Learning with Vision Transformers) is a cutting-edge model designed for open vocabulary object detection. Unlike traditional models, OWL-ViT combines vision transformers with text embeddings, enabling it to:\\n\\n\n",
    "- Understand textual descriptions of objects, even if it hasn't seen them during training.\n",
    "- Detect and classify objects based on descriptive input, making it suitable for diverse applications.\n",
    "- Perform zero-shot learning by generalizing to new object classes without additional training.\\n\\n\"\n",
    "\n",
    "**Steps in Using OWL-ViT:**\n",
    "1. Model Initialization**: Set up the OWL-ViT model.\n",
    "2. Text Input for Object Descriptions: Provide descriptive prompts (e.g., 'a red car' or 'a black cat to guide detection.\n",
    "3. Inference and Visualization: Process an image or video, detect objects based on text descriptions and visualize results.\\n\\n\"\n",
    "\n",
    "OWL-ViT excels in scenarios where predefined object classes are insufficient, such as detecting rare or domain-specific objects.\n",
    "\n",
    "**Support Material**:\n",
    "- https://huggingface.co/docs/transformers/en/model_doc/owlvit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load image processor for 'google/owlvit-base-patch32'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'google/owlvit-base-patch32' is the correct path to a directory containing a preprocessor_config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_processing_base.py:354\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m     resolved_image_processor_files \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    334\u001b[0m         resolved_file\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m [image_processor_file, PROCESSOR_NAME]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     ]\n\u001b[0;32m--> 354\u001b[0m     resolved_image_processor_file \u001b[38;5;241m=\u001b[39m \u001b[43mresolved_image_processor_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OwlViTProcessor, OwlViTForObjectDetection\n\u001b[1;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages/street_scene.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mOwlViTProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/owlvit-base-patch32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m OwlViTForObjectDetection\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/owlvit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m text_labels \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma person on the floor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma church \u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/processing_utils.py:1394\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1392\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m-> 1394\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1395\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/processing_utils.py:1453\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1451\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_possibly_dynamic_module(class_name)\n\u001b[0;32m-> 1453\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_processing_base.py:201\u001b[0m, in \u001b[0;36mImageProcessingMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 201\u001b[0m image_processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_dict(image_processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/image_processing_base.py:361\u001b[0m, in \u001b[0;36mImageProcessingMixin.get_image_processor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    362\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load image processor for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m it from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m same name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_processor_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m         )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;66;03m# Load image_processor dict\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(resolved_image_processor_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load image processor for 'google/owlvit-base-patch32'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'google/owlvit-base-patch32' is the correct path to a directory containing a preprocessor_config.json file"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "\n",
    "image = Image.open(\"images/street_scene.jpg\")\n",
    "\n",
    "processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "\n",
    "\n",
    "text_labels = [[\"a person on the floor\", \"a church \"]]\n",
    "\n",
    "inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\n",
    "target_sizes = torch.tensor([(image.height, image.width)])\n",
    "\n",
    "# Convert outputs (bounding boxes and class logits) to Pascal VOC format (xmin, ymin, xmax, ymax)\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs=outputs, target_sizes=target_sizes, threshold=0.1, text_labels=text_labels\n",
    ")\n",
    "# Retrieve predictions for the first image for the corresponding text queries\n",
    "result = results[0]\n",
    "boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n",
    "\n",
    "for box, score, text_label in zip(boxes, scores, text_labels):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_outputs(output):\n",
    "    input_scores = [x[\"score\"] for x in output]\n",
    "    input_labels = [x[\"label\"] for x in output]\n",
    "    input_boxes = []\n",
    "    for i in range(len(output)):\n",
    "        input_boxes.append([*output[i][\"box\"].values()])\n",
    "    input_boxes = [input_boxes]\n",
    "    return input_scores, input_labels, input_boxes\n",
    "\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
    "    )\n",
    "\n",
    "\n",
    "def show_boxes_and_labels_on_image(raw_image, boxes, labels, scores):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(raw_image)\n",
    "    for i, box in enumerate(boxes):\n",
    "        show_box(box, plt.gca())\n",
    "        plt.text(\n",
    "            x=box[0],\n",
    "            y=box[1] - 12,\n",
    "            s=f\"{labels[i]}: {scores[i]:,.4f}\",\n",
    "            c=\"beige\",\n",
    "            path_effects=[pe.withStroke(linewidth=4, foreground=\"darkgreen\")],\n",
    "        )\n",
    "    plt.axis(\"on\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#input_scores, input_labels, input_boxes = preprocess_outputs(output)\n",
    "\n",
    "# Show the image with the bounding boxes\n",
    "#show_boxes_and_labels_on_image(\n",
    "#    image,\n",
    "#    input_boxes[0],\n",
    "#    input_labels,\n",
    "#    input_scores\n",
    "#)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
