{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. OpenAI VLM (GPT) - Basics\n",
    "This section demonstrates the basic usage of OpenAI's Vision Language Model (VLM) capabilities using GPT-4.1.\n",
    "We will use the OpenAI API to analyze an image and provide detailed textual insights.\n",
    "\n",
    "**Support Material**\n",
    "\n",
    "- https://platform.openai.com/docs/quickstart\n",
    "- https://platform.openai.com/docs/guides/text\n",
    "- https://platform.openai.com/docs/guides/images-vision?api-mode=chat\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv  \n",
    "import base64\n",
    "import json\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# Instantiate client; API key read from environment (.env)\n",
    "openAIclient = openai.OpenAI()\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a busy urban street scene with a mix of people and vehicles. In the foreground, there is a person\n",
      "lying on the ground while another person sits cross-legged nearby using a smartphone or a similar device. Several\n",
      "pigeons are scattered around them. On a bench close by, there is an older man in a suit and a younger woman reading a\n",
      "newspaper. Another woman is walking by, holding a phone and a drink.   In the background, there is typical city traffic,\n",
      "including a taxi, a car, a motorbike, and a person playing a guitar while walking. The buildings have classic and modern\n",
      "architecture, and the overall atmosphere is lively with a mix of pedestrians and vehicles. The sun is either rising or\n",
      "setting, casting a warm light over the scene.\n"
     ]
    }
   ],
   "source": [
    "#basic call to gpt with prompt and image\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1.1 Structured Output\n",
    "Here, we expand upon the VLM example to request structured outputs. This approach allows for extracting \n",
    "well-organized information from images in a machine-readable format, such as JSON.\n",
    "\n",
    "**Support Material**:\n",
    "- https://platform.openai.com/docs/guides/structured-outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={ \"type\": \"json_object\" },# NEW!!\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse the json in a dict structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json.loads(returnValue)\n",
    "#json. loads() converts JSON strings to Python objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can access specific infos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Young person with short hair, wearing a green jacket and shorts, focused on a tablet or phone'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"foreground\"][\"people\"][0][\"description\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# JSON Schema for Controlled Structured Outputs\n",
    "In this section, we define a JSON schema for a more controlled and specific output from the model. \n",
    "Using this schema, we can ensure the model adheres to predefined data types and structures while describing images.In this case we will provide the json schema directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",    \n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]\n",
    "                    }}},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image_extraction = json.loads(returnValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'position': 'sitting on the sidewalk',\n",
       "  'age': 16,\n",
       "  'activity': 'using a smartphone',\n",
       "  'gender': 'male'},\n",
       " {'position': 'lying on the sidewalk',\n",
       "  'age': 18,\n",
       "  'activity': 'resting or sleeping',\n",
       "  'gender': 'male'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 65,\n",
       "  'activity': 'thinking or resting',\n",
       "  'gender': 'male'},\n",
       " {'position': 'sitting on a bench',\n",
       "  'age': 25,\n",
       "  'activity': 'reading a newspaper',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk',\n",
       "  'age': 20,\n",
       "  'activity': 'looking at a phone',\n",
       "  'gender': 'female'},\n",
       " {'position': 'riding a motorcycle',\n",
       "  'age': 30,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'male'},\n",
       " {'position': 'walking on the street',\n",
       "  'age': 28,\n",
       "  'activity': 'playing guitar',\n",
       "  'gender': 'male'},\n",
       " {'position': 'riding a scooter',\n",
       "  'age': 27,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'female'},\n",
       " {'position': 'walking on the sidewalk',\n",
       "  'age': 35,\n",
       "  'activity': 'walking',\n",
       "  'gender': 'female'},\n",
       " {'position': 'inside a taxi car',\n",
       "  'age': 40,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'male'},\n",
       " {'position': 'inside a car',\n",
       "  'age': 45,\n",
       "  'activity': 'driving',\n",
       "  'gender': 'female'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_image_extraction[\"people\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively: \n",
    "\n",
    "\n",
    "OpenAI SDKs for Python and JavaScript also make it easy to define object schemas using Pydantic and Zod respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    position: str \n",
    "    age: int \n",
    "    activity: str \n",
    "    gender: str\n",
    "\n",
    "\n",
    "class ImageExtraction(BaseModel):\n",
    "    number_of_people: int \n",
    "    atmosphere: str \n",
    "    hour_of_the_day: int \n",
    "    people: list[Person] \n",
    "\n",
    "completion = openAIclient.beta.chat.completions.parse(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": \"describe the image in detail\"}\n",
    "    ],\n",
    "    response_format=ImageExtraction,\n",
    ")\n",
    "\n",
    "output_image_extraction = completion.choices[0].message.parsed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then integrate the extracted information in full or partially in a new prompt for a new extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alert service prompt \n",
    "\n",
    "alert_sys_prompt = \" you are an experienced first aid paramedical\"\n",
    "alert_prompt= \"\"\"Extract from the following scene analysis give to you in json format, \n",
    "if anyone might be in danger and if the Child Hospital or normal Hospital should be alerted. \n",
    "Give the a concise answer\n",
    "The situation is given to you from this object: \"\"\" + str(output_image_extraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one appears to be in immediate danger based on the scene description. No alerts to either Child Hospital or normal\n",
      "Hospital are necessary.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": alert_prompt},\n",
    "        {\"role\": \"user\", \"content\": alert_prompt}\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The youngest person in the list is a 16-year-old male sitting on the sidewalk using a smartphone. In the picture, he is\n",
      "the person sitting on the ground near the bottom right corner, looking at a phone/tablet device.  Since you only\n",
      "provided the image and list, I can't directly extract the exact coordinates. Please provide the bounding box or location\n",
      "info, or a specific mark, and I will convert that into normalized coordinates for you. Alternatively, if you want me to\n",
      "estimate his bounding box from the image, please confirm.\n"
     ]
    }
   ],
   "source": [
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Considering this list of people\"+str(output_image_extraction[\"people\"])+\".Identify the youngest in the picture I provide and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# Wrap the text to a specified width\n",
    "\n",
    "response = str(completion.choices[0].message.content)\n",
    "print(textwrap.fill(response, width=120))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Google VLM (Gemini)\n",
    "This section demonstrates the use of Google's Vision Language Model, Gemini. \n",
    "We explore basic text generation as well as its ability to analyze images and provide relevant outputs.\n",
    "\n",
    "**Support Material**:\n",
    "- https://ai.google.dev/gemini-api/docs/quickstart\n",
    "- https://ai.google.dev/gemini-api/docs/text-generation\n",
    "- https://ai.google.dev/gemini-api/docs/image-understanding\n",
    "- https://ai.google.dev/gemini-api/docs/structured-output?example=recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dotenv import load_dotenv  \n",
    "from google import genai\n",
    "from PIL import Image\n",
    "import textwrap\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# Instantiate the Gemini client; the key is read from the environment (.env).\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Path to your image\n",
    "img = \"images/street_scene.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine a smart computer that *learns* from experience, much like a person does. It takes in lots of information, then\n",
      "uses what it's learned to understand things, answer questions, or help you with tasks.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\", contents=\"Explain how AI works to a 90 years old. in few words\"\n",
    ")\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and with images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image captures a vibrant and somewhat surreal urban street scene during what appears to be either early morning or\n",
      "late afternoon, given the warm, golden light bathing the cityscape. The scene is bustling with a mix of activity, from\n",
      "fast-moving traffic to a variety of pedestrians engaged in different actions.  **Overall Composition and Lighting:** The\n",
      "composition uses a wide-angle perspective, placing a prominent black and white striped crosswalk in the lower-center\n",
      "foreground, acting as a visual anchor. The lighting is distinct, casting long shadows and illuminating the buildings and\n",
      "street with a soft, warm glow, particularly noticeable on the brick buildings to the left and the distant skyline. This\n",
      "golden hour light adds depth and atmosphere to the busy setting.  **Foreground - Sidewalk Activity:** On the immediate\n",
      "left sidewalk, a large, rustic-looking wooden pot overflowing with bright red geraniums adds a splash of color. Nearby,\n",
      "a young person with short brown hair, wearing a green jacket and grey shorts, sits cross-legged on the pavement,\n",
      "engrossed in a tablet or phone. Further to the right, another young person in a red hoodie and blue jeans lies flat on\n",
      "their back on the sidewalk, eyes seemingly closed or looking upwards, their head pointing towards the viewer. Several\n",
      "pigeons are scattered around this area, pecking at the ground or standing still, seemingly unbothered by the human\n",
      "presence.  On the right side of the foreground, a sturdy wooden park bench with black metal supports is occupied by two\n",
      "people. An older man, dressed in a dark suit and glasses, sits on the left side of the bench, his hand thoughtfully\n",
      "touching his chin. Next to him, a woman with blonde hair, wearing a red and white striped top and blue jeans, is\n",
      "absorbed in reading a newspaper or magazine. To the right of the bench, a young woman with long dark hair, wearing a\n",
      "pink top, denim shorts, and white sneakers, stands holding a small plate or bowl, looking towards the street or the\n",
      "bench.  **Midground - Street and Moving Figures:** The street is a hive of motion. A silver car, possibly a taxi given\n",
      "the blurred yellow object on its roof, speeds across the crosswalk from left to right, creating a strong sense of motion\n",
      "blur. Behind it, a motorcyclist in a black leather suit and white helmet rides a vintage-style motorcycle, also moving\n",
      "across the crosswalk, with visible motion blur on the wheels. Further back and to the right, a man in a dark suit and\n",
      "fedora strides purposefully across the street, remarkably carrying and seemingly playing an acoustic guitar as he walks.\n",
      "More cars are visible in the midground: an orange car blurred behind the motorcyclist, and a grey SUV further back to\n",
      "the left. On the right side of the street, a woman on a light-colored scooter or moped, wearing a helmet and dark\n",
      "clothes, drives towards the left. A tall traffic light with three amber signals hangs over the right side of the street.\n",
      "**Background - Cityscape:** The background features a dense urban landscape. On the left, an old, multi-story brick\n",
      "building with several arched windows and dark green awnings over storefronts dominates. Across the street, a mix of\n",
      "modern skyscrapers with glass facades and older, more traditional buildings lines the street, stretching into the hazy\n",
      "distance. A prominent spire of a church or older building can be seen nestled among the modern structures in the center-\n",
      "left background. Pedestrians are visible walking on the sidewalks in the distance, adding to the urban feel, though they\n",
      "are less distinct due to the distance and the soft lighting.  The overall impression is a vibrant, dynamic urban\n",
      "snapshot, full of individual narratives unfolding simultaneously, from the mundane to the slightly eccentric, all bathed\n",
      "in a beautiful, warm light.\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(img)\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details\\n\"],\n",
    "                                          )\n",
    "\n",
    "print(textwrap.fill(response.text, width=120))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here we can extract structured output (Gemini actually prefers pydantic syntax - let's see what happens with a schema as before)-> check limitations in https://ai.google.dev/gemini-api/docs/structured-output?example=recipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"scene_description\": \"A bustling city street scene bathed in warm, golden hour lighting, depicting various individuals engaged in different activities alongside moving vehicles and diverse architecture. The foreground features pedestrians on a crosswalk and sidewalk, while the background showcases a dynamic urban skyline.\",\n",
      "  \"time_of_day\": \"Daytime, specifically late afternoon or early evening, indicated by the warm, golden glow of the sun.\",\n",
      "  \"overall_mood\": \"Lively and dynamic, capturing a 'slice of life' in an urban setting with a mix of activity and repose.\",\n",
      "  \"elements\": [\n",
      "    \"people\",\n",
      "    \"vehicles\",\n",
      "    \"buildings\",\n",
      "    \"natural elements\",\n",
      "    \"street furniture and infrastructure\"\n",
      "  ],\n",
      "  \"details\": [\n",
      "    \"A young person with short brown hair, wearing a green jacket and shorts, sits on the left side of the sidewalk, engrossed in a tablet or phone.\",\n",
      "    \"A young man in a red hoodie and blue jeans lies flat on his back on the sidewalk near the center foreground, surrounded by pigeons.\",\n",
      "    \"On a wooden park bench on the right, an older man in a dark suit sits pensively, while a woman with blonde hair in a striped red shirt reads a newspaper beside him.\",\n",
      "    \"A young woman with long dark hair, wearing a pink t-shirt and light shorts, walks casually on the sidewalk to the right, holding a small item in her hand.\",\n",
      "    \"Various vehicles are in motion on the street, including a silver sedan (possibly a taxi) and an orange car, both showing motion blur as they cross the pedestrian crosswalk.\",\n",
      "    \"A person in a white helmet and black leather rides a motorcycle across the crosswalk, exhibiting motion blur.\",\n",
      "    \"A man in dark attire and a hat walks across the crosswalk, carrying and possibly playing an acoustic guitar.\",\n",
      "    \"A woman in dark clothing rides a light-colored scooter or moped across the crosswalk.\",\n",
      "    \"The street features a wide pedestrian crosswalk with black and white stripes.\",\n",
      "    \"An old brick building with multiple windows and green awnings stands on the left side of the street.\",\n",
      "    \"A distinctive church-like building with a spire is visible in the mid-background, flanked by modern skyscrapers and high-rise buildings.\",\n",
      "    \"A traffic light with three yellow/orange lights is visible on a pole to the right, illuminated by the golden sunlight.\",\n",
      "    \"Several pigeons are scattered on the sidewalk near the man lying down and the bench.\",\n",
      "    \"A potted plant with red flowers sits on the sidewalk in the foreground, near the crosswalk.\",\n",
      "    \"Other blurred figures of people and cars can be seen in the background, adding to the city's hustle and bustle.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_schema = {\n",
    "                    \"name\": \"img_extract\",\n",
    "                    \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"numberOfPeople\": {\n",
    "                        \"type\":\"integer\",\n",
    "                        \"description\": \"The total number of people in the environment\",\n",
    "                        \"minimum\": 0\n",
    "                        },\n",
    "                        \"atmosphere\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Description of the atmosphere, e.g., calm, lively, etc.\"\n",
    "                        },\n",
    "                        \"hourOfTheDay\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The hour of the day in 24-hour format\",\n",
    "                        \"minimum\": 0,\n",
    "                        \"maximum\": 23\n",
    "                        },\n",
    "                        \"people\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"description\": \"List of people and their details\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                            \"position\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Position of the person in the environment, e.g., standing, sitting, etc.\"\n",
    "                            },\n",
    "                            \"age\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Age of the person\",\n",
    "                                \"minimum\": 0\n",
    "                            },\n",
    "                            \"activity\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Activity the person is engaged in, e.g., reading, talking, etc.\"\n",
    "                            },\n",
    "                            \"gender\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Gender of the person\",\n",
    "                                \"enum\": [\"male\", \"female\", \"non-binary\", \"other\", \"prefer not to say\"]\n",
    "                            }\n",
    "                            },\n",
    "                            \"required\": [\"position\", \"age\", \"activity\", \"gender\"]\n",
    "                        }\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"numberOfPeople\", \"atmosphere\", \"hourOfTheDay\", \"people\"]}}\n",
    "\n",
    "\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Describe the scene in details, follwoing exactly the given json schema\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it match your schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use Gemini to detect an object in the image and get its coordinates:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'box_2d': [631, 287, 853, 396]}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Identify the youngest in the picture and give me back their coordinates. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000.\"\n",
    "\n",
    "\n",
    "config={\"response_mime_type\": \"application/json\"}\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[img, prompt],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "bounding_boxes = json.loads(response.text)\n",
    "print(bounding_boxes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini2+ was trained specifically for object detection/ segmentation tasks. More details: https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Extract Structured Infos from Hand-written note - GPT & Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try **not** to extract structured information from a handwritten note (e.g., `prescription1.jpg`) using **both models**.\n",
    "\n",
    "Consider the file: `/images/prescription1.jpg`.  \n",
    "Have a look at it.\n",
    "\n",
    "### JSON Schema\n",
    "Let’s define a JSON schema for the extraction task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_prescription = {\n",
    " \"name\": \"prescription_extract\",\n",
    "\"schema\": {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"doctor_name\": { \"type\": \"string\" },\n",
    "    \"patient_name\": { \"type\": \"string\" },\n",
    "    \"patient_dob\": { \"type\": \"string\" },\n",
    "    \"meds\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"name\": { \"type\": \"string\" },\n",
    "          \"dose\": { \"type\": \"string\" },\n",
    "          \"frequency\": { \"type\": \"string\" },\n",
    "          \"instructions\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"name\"]\n",
    "      }\n",
    "    },\n",
    "    \"signature\": { \"type\": \"boolean\" }\n",
    "  },\n",
    "  \"required\": [\"doctor_name\", \"patient_name\", \"meds\"]\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract structured infos using Gemini: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doctor_name\": \"Dr. Markus Hütter\",\n",
      "  \"doctor_specialty\": \"Chirurg\",\n",
      "  \"patient_name\": \"Claudie Fischer\",\n",
      "  \"patient_dob\": \"1.4.1978\",\n",
      "  \"patient_gender\": \"f\",\n",
      "  \"medication\": \"Ibuprofen\",\n",
      "  \"dosage\": \"3x 400mg\",\n",
      "  \"instructions\": \"nach dem Essen\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "im = Image.open(\"images/prescription1.jpg\")\n",
    "\n",
    "config={\n",
    "        \"response_mime_type\": \"application/json\",\n",
    "        \"response_json_schema\": json_schema_prescription,\n",
    "    }\n",
    "\n",
    "\n",
    "response = client.models.generate_content(model=\"gemini-2.5-flash\",\n",
    "                                          contents=[im, \"Extract infos from image, follwoing the given json schema.\\n\"],\n",
    "                                          config=config\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the output is **not valid JSON** and contains extra strings, it must be **parsed** before it can be loaded into a Python dict.  \n",
    "Below is an example helper function that does this.\n",
    "\n",
    "> **Note:** Since Gemini returns a Pydantic model, you *could* use Pydantic methods to handle parsing.  \n",
    "> We avoid that here to keep the workflow generally compatible across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json \n",
    "def parse_json_in_output(output):\n",
    "    \"\"\"\n",
    "    Extracts and converts JSON-like data from the given text output to a Python dictionary.\n",
    "    \n",
    "    Args:\n",
    "        output (str): The text output containing the JSON data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The parsed JSON data as a Python dictionary.\n",
    "    \"\"\"\n",
    "    # Regex to extract JSON-like portion\n",
    "    json_match = re.search(r\"\\{.*?\\}\", output, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_str = json_match.group(0)\n",
    "        # Fix single quotes and ensure proper JSON formatting\n",
    "        json_str = json_str.replace(\"'\", '\"')  # Replace single quotes with double quotes\n",
    "        try:\n",
    "            # Convert the fixed JSON string into a dictionary\n",
    "            json_data = json.loads(json_str)\n",
    "            return json_data\n",
    "        except json.JSONDecodeError:\n",
    "            return \"The extracted JSON is still not valid after formatting.\"\n",
    "    else:\n",
    "        return \"No JSON data found in the given output.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(parse_json_in_output(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doctor_name': 'Dr. Markus Hütter',\n",
       " 'doctor_specialty': 'Chirurg',\n",
       " 'patient_name': 'Claudie Fischer',\n",
       " 'patient_dob': '1.4.1978',\n",
       " 'patient_gender': 'f',\n",
       " 'medication': 'Ibuprofen',\n",
       " 'dosage': '3x 400mg',\n",
       " 'instructions': 'nach dem Essen'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same with GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = \"images/prescription1.jpg\"\n",
    "\n",
    "completion = openAIclient.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"you are a careful observer. the response should be in json format\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Describe the image in detail\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{encode_image(im)}\",\n",
    "                        #\"detail\": \"low\"\n",
    "                    }\n",
    "                },\n",
    "            ]}\n",
    "    ],\n",
    "    response_format={\n",
    "                \"type\": \"json_schema\",   \"json_schema\": json_schema_prescription},\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "returnValue = completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"doctor_name\":\"Dr. Markus Müller\",\"patient_name\":\"Claudia Fischer\",\"patient_dob\":\"1.4.1978\",\"meds\":[{\"name\":\"Ibuprofen\",\"dose\":\"400 mg\",\"frequency\":\"3x\",\"instructions\":\"nach dem Essen\"}],\"signature\":true}'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any difference wiht the output of Gemini vs your schema? \n",
    "\n",
    "No need for parsing now. We load the json in a python dict structure with json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doctor_name': 'Dr. Markus Müller', 'patient_name': 'Claudia Fischer', 'patient_dob': '1.4.1978', 'meds': [{'name': 'Ibuprofen', 'dose': '400 mg', 'frequency': '3x', 'instructions': 'nach dem Essen'}], 'signature': True}\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(returnValue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
